# LLM Chat in Terminal

Note:
In order to use this code, you will need to clone and compile the [llama.cpp Github Repo](https://github.com/ggerganov/llama.cpp) locally on your system. Navigate to the link to know the steps of compiling.

Make sure the `main.py` file is in the same directory as the `llama.cpp` cloned repository.

To run (after compiling llama.cpp):

```shell
$ python main.py
```

If you have more CPU/GPU RAM (or VRAM), you can also change the code (conversation history) accordingly.

Happy chatting!
